{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw-NKhTBScq2"
      },
      "outputs": [],
      "source": [
        "# DGL is reliable in Colab for heterogeneous graphs.\n",
        "!pip -q install dgl torch torchvision torchaudio\n",
        "# Progress bars\n",
        "!pip -q install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) IMPORTS & SEEDING\n",
        "\n",
        "import os, json, math, random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import dgl\n",
        "from dgl.nn.pytorch import HeteroGraphConv, GraphConv\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device"
      ],
      "metadata": {
        "id": "UiqeDRMJSzPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) DATA LOADING\n",
        "\n",
        "DATA_PATH = \"/content/dataset.json\"  # change it accorling to uploaded path\n",
        "\n",
        "def random_features(dim=8):\n",
        "    return [float(np.round(random.random(), 3)) for _ in range(dim)]\n",
        "\n",
        "def generate_graph(graph_id, node_types, edge_types,\n",
        "                   num_nodes=300, num_edges=800, feat_dim=8):\n",
        "    # nodes\n",
        "    nodes = []\n",
        "    for i in range(num_nodes):\n",
        "        ntype = random.choice(node_types)\n",
        "        nodes.append({\n",
        "            \"id\": f\"{graph_id}_N{i}\",\n",
        "            \"type\": ntype,\n",
        "            \"features\": random_features(feat_dim)\n",
        "        })\n",
        "    # edges\n",
        "    edges = []\n",
        "    for _ in range(num_edges):\n",
        "        src = random.choice(nodes)\n",
        "        tgt = random.choice(nodes)\n",
        "        while tgt[\"id\"] == src[\"id\"]:\n",
        "            tgt = random.choice(nodes)\n",
        "        relation = random.choice(edge_types)\n",
        "        edges.append({\n",
        "            \"source\": src[\"id\"],\n",
        "            \"target\": tgt[\"id\"],\n",
        "            \"relation\": relation\n",
        "        })\n",
        "    return {\n",
        "        \"graph_id\": graph_id,\n",
        "        \"node_types\": node_types,\n",
        "        \"edge_types\": edge_types,\n",
        "        \"nodes\": nodes,\n",
        "        \"edges\": edges\n",
        "    }\n",
        "\n",
        "def load_or_make_dataset(path=DATA_PATH, feat_dim=8):\n",
        "    if os.path.exists(path):\n",
        "        print(f\"Loading dataset from: {path}\")\n",
        "        with open(path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    else:\n",
        "        print(\"Dataset file not found â€” generating 5 synthetic graphs...\")\n",
        "        graphs = [\n",
        "            generate_graph(\"G1\", [\"patient\", \"symptom\", \"disease\"], [\"has_symptom\", \"diagnosed_with\"], feat_dim=feat_dim),\n",
        "            generate_graph(\"G2\", [\"gene\", \"protein\", \"pathway\"], [\"encodes\", \"involved_in\"], feat_dim=feat_dim),\n",
        "            generate_graph(\"G3\", [\"drug\", \"target\", \"disease\"], [\"binds_to\", \"treats\"], feat_dim=feat_dim),\n",
        "            generate_graph(\"G4\", [\"treatment\", \"outcome\", \"disease\"], [\"leads_to\", \"associated_with\"], feat_dim=feat_dim),\n",
        "            generate_graph(\"G5\", [\"microbe\", \"metabolite\", \"disease\"], [\"produces\", \"affects\"], feat_dim=feat_dim)\n",
        "        ]\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(graphs, f, indent=2)\n",
        "        print(f\"Saved synthetic dataset to: {path}\")\n",
        "        return graphs\n",
        "\n",
        "raw_graphs = load_or_make_dataset()\n"
      ],
      "metadata": {
        "id": "ZZ_UOoFfS4uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) BUILD DGL HETEROGRAPHS FROM JSON\n",
        "\n",
        "\n",
        "def build_dgl_graph_from_json(jg):\n",
        "    \"\"\"\n",
        "    jg: one JSON graph dict with keys:\n",
        "        graph_id, node_types, edge_types, nodes, edges\n",
        "    Returns: (g, ntype2id, feats_dict, node_id_map)\n",
        "        g: DGLHeteroGraph\n",
        "        ntype2id: map node_type -> list(node_ids in order)\n",
        "        feats_dict: dict of {ntype: tensor features [N_t, d]}\n",
        "        node_id_map: global_id_str -> (ntype, idx_in_type)\n",
        "    \"\"\"\n",
        "    node_types = jg[\"node_types\"]\n",
        "    edge_types = jg[\"edge_types\"]\n",
        "    nodes = jg[\"nodes\"]\n",
        "    edges = jg[\"edges\"]\n",
        "\n",
        "    # Separate node IDs by type\n",
        "    ntype2nodes = defaultdict(list)\n",
        "    for n in nodes:\n",
        "        ntype2nodes[n[\"type\"]].append(n)\n",
        "\n",
        "    # Assign per-type indices and gather features\n",
        "    ntype2id = {}\n",
        "    feats = {}\n",
        "    node_id_map = {}  # str_id -> (ntype, idx)\n",
        "    for ntype, nlist in ntype2nodes.items():\n",
        "        ntype2id[ntype] = []\n",
        "        feat_list = []\n",
        "        for idx, obj in enumerate(nlist):\n",
        "            ntype2id[ntype].append(obj[\"id\"])\n",
        "            node_id_map[obj[\"id\"]] = (ntype, idx)\n",
        "            feat_list.append(obj[\"features\"])\n",
        "        feats[ntype] = torch.tensor(np.array(feat_list, dtype=np.float32))\n",
        "\n",
        "    # Build typed edge lists\n",
        "    etype2edges = defaultdict(lambda: ([], []))  # (src_idx_list, dst_idx_list)\n",
        "    for e in edges:\n",
        "        r = e[\"relation\"]\n",
        "        u = e[\"source\"]\n",
        "        v = e[\"target\"]\n",
        "        if u not in node_id_map or v not in node_id_map:\n",
        "            continue\n",
        "        u_type, u_idx = node_id_map[u]\n",
        "        v_type, v_idx = node_id_map[v]\n",
        "        etype = (u_type, r, v_type)\n",
        "        etype2edges[etype][0].append(u_idx)\n",
        "        etype2edges[etype][1].append(v_idx)\n",
        "\n",
        "    # Build heterograph\n",
        "    data_dict = {}\n",
        "    for etype, (src_list, dst_list) in etype2edges.items():\n",
        "        src_nt, rel, dst_nt = etype\n",
        "        data_dict[etype] = (torch.tensor(src_list), torch.tensor(dst_list))\n",
        "\n",
        "    g = dgl.heterograph(data_dict)\n",
        "\n",
        "    # Attach node features per type\n",
        "    for ntype in g.ntypes:\n",
        "        if ntype in feats:\n",
        "            g.nodes[ntype].data[\"h\"] = feats[ntype]\n",
        "        else:\n",
        "            # If a type has no nodes\n",
        "            pass\n",
        "    return g, ntype2id, feats, node_id_map\n",
        "\n",
        "dgl_graphs = []\n",
        "meta_domains = []  # just keep graph_id meta\n",
        "for jg in raw_graphs:\n",
        "    g, nmap, feats, nidmap = build_dgl_graph_from_json(jg)\n",
        "    dgl_graphs.append({\"graph_id\": jg[\"graph_id\"], \"g\": g, \"ntype2ids\": nmap, \"feats\": feats, \"nidmap\": nidmap})\n",
        "    meta_domains.append(jg[\"graph_id\"])\n",
        "\n",
        "print(\"Built heterographs:\", [x[\"graph_id\"] for x in dgl_graphs])\n",
        "print(\"Node types per graph:\", dgl_graphs[0][\"g\"].ntypes)\n",
        "print(\"Edge types per graph:\", dgl_graphs[0][\"g\"].etypes[:5], \"...\")"
      ],
      "metadata": {
        "id": "mPvwrggKTV7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4) FEW-SHOT TASK DEFINITION (labels on disease nodes)\n",
        "\n",
        "C = 5          # number of disease classes per graph\n",
        "FEAT_DIM = next(iter(dgl_graphs[0][\"feats\"].values())).shape[1]\n",
        "\n",
        "def assign_pseudo_labels_for_disease(gpack, num_classes=C):\n",
        "    \"\"\"\n",
        "    Assigns pseudo labels [0..C-1] to disease nodes uniformly/randomly.\n",
        "    Returns dict with:\n",
        "        'labels': tensor [num_disease_nodes]\n",
        "        'idxs': list of node indices for disease nodes\n",
        "    \"\"\"\n",
        "    g = gpack[\"g\"]\n",
        "    if \"disease\" not in g.ntypes:\n",
        "        return {\"labels\": None, \"idxs\": []}\n",
        "\n",
        "    N = g.num_nodes(\"disease\")\n",
        "    if N == 0:\n",
        "        return {\"labels\": None, \"idxs\": []}\n",
        "\n",
        "    labels = torch.randint(low=0, high=num_classes, size=(N,))\n",
        "    return {\"labels\": labels, \"idxs\": list(range(N))}\n",
        "\n",
        "# Attach labels to each graph (for disease nodes only)\n",
        "for gp in dgl_graphs:\n",
        "    gp[\"disease_labels\"] = assign_pseudo_labels_for_disease(gp, C)\n"
      ],
      "metadata": {
        "id": "WRb0E7VYTkfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5) CH-MHGNN-STYLE ENCODER\n",
        "\n",
        "class CHMHGNNEncoder(nn.Module):\n",
        "    def __init__(self, in_dims_dict, hidden_dim=64, out_dim=64):\n",
        "        super().__init__()\n",
        "        # Build per-rel GraphConv for layer1 and layer2\n",
        "        self.layer1 = HeteroGraphConv(\n",
        "            { et: GraphConv(in_feats=in_dims_dict[et[0]], out_feats=hidden_dim)\n",
        "              for et in [] }, aggregate='sum'\n",
        "        )\n",
        "        self.layer2 = HeteroGraphConv(\n",
        "            { et: GraphConv(in_feats=hidden_dim, out_feats=out_dim)\n",
        "              for et in [] }, aggregate='sum'\n",
        "        )\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "        # Since DGL needs modules for each existing edge type at runtime,\n",
        "        # we will lazily (re)build conv dicts once we see the concrete graph.\n",
        "\n",
        "        self._cached_etypes = None\n",
        "        self._built1 = None\n",
        "        self._built2 = None\n",
        "\n",
        "        # Final projection per node type for alignment\n",
        "        self.proj = nn.ModuleDict()\n",
        "\n",
        "    def _ensure_layers(self, g, in_dims_dict, hidden_dim=64, out_dim=64):\n",
        "        etypes = g.canonical_etypes\n",
        "        if self._cached_etypes == etypes:\n",
        "            return  # nothing to do\n",
        "\n",
        "        # Layer1 per etype\n",
        "        mods1 = {}\n",
        "        for (src, rel, dst) in etypes:\n",
        "            mods1[(src, rel, dst)] = GraphConv(in_feats=in_dims_dict[src], out_feats=hidden_dim)\n",
        "        self.layer1 = HeteroGraphConv(mods1, aggregate='sum')\n",
        "\n",
        "        # Layer2 per etype\n",
        "        mods2 = {}\n",
        "        for (src, rel, dst) in etypes:\n",
        "            mods2[(src, rel, dst)] = GraphConv(in_feats=hidden_dim, out_feats=out_dim)\n",
        "        self.layer2 = HeteroGraphConv(mods2, aggregate='sum')\n",
        "\n",
        "        # Projections per node type\n",
        "        self.proj = nn.ModuleDict()\n",
        "        for ntype in g.ntypes:\n",
        "            self.proj[ntype] = nn.Linear(out_dim, out_dim)\n",
        "\n",
        "        self._cached_etypes = etypes\n",
        "\n",
        "    def forward(self, g, feats_dict):\n",
        "        \"\"\"\n",
        "        feats_dict: {ntype: tensor [N_t, d_in_t]}\n",
        "        Returns:\n",
        "            z_dict: {ntype: tensor [N_t, out_dim]}\n",
        "        \"\"\"\n",
        "        # Infer in_dims for the current graph\n",
        "        in_dims = {nt: feats_dict[nt].shape[1] for nt in feats_dict.keys()}\n",
        "        self._ensure_layers(g, in_dims, hidden_dim=64, out_dim=64)\n",
        "\n",
        "        h_dict = self.layer1(g, feats_dict)\n",
        "        h_dict = {k: self.act(v) for k, v in h_dict.items()}\n",
        "        h_dict = self.layer2(g, h_dict)\n",
        "        # projections for alignment\n",
        "        z_dict = {nt: self.proj[nt](h) for nt, h in h_dict.items()}\n",
        "        return z_dict\n",
        "\n",
        "# Initialize a single encoder to be shared across tasks (graphs)\n",
        "# Figure out initial input dims per node type from the first graph\n",
        "# Fallback: if a node type missing in graph 0, weâ€™ll infer at runtime\n",
        "all_ntypes = set()\n",
        "for gp in dgl_graphs:\n",
        "    all_ntypes.update(gp[\"g\"].ntypes)\n",
        "\n",
        "# Guess per-type input dims (8 by default if not present in graph0)\n",
        "in_dims_dict = {}\n",
        "for nt in all_ntypes:\n",
        "    # try to find first graph that has this node type\n",
        "    d_in = None\n",
        "    for gp in dgl_graphs:\n",
        "        if nt in gp[\"feats\"]:\n",
        "            d_in = gp[\"feats\"][nt].shape[1]\n",
        "            break\n",
        "    in_dims_dict[nt] = d_in if d_in is not None else 8\n",
        "\n",
        "encoder = CHMHGNNEncoder(in_dims_dict, hidden_dim=64, out_dim=64).to(device)"
      ],
      "metadata": {
        "id": "BY32gSzDT0c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6) PROTOTYPICAL EPISODE (FEW-SHOT)\n",
        "\n",
        "def sample_support_query(labels, K=5, max_query_per_class=20):\n",
        "    \"\"\"\n",
        "    labels: tensor [N] with class ids 0..C-1\n",
        "    returns: dict { 'support_idx', 'query_idx' }\n",
        "    \"\"\"\n",
        "    support_idx, query_idx = [], []\n",
        "    labels = labels.cpu()\n",
        "    classes = torch.unique(labels).tolist()\n",
        "    for c in classes:\n",
        "        idx = (labels == c).nonzero(as_tuple=True)[0].tolist()\n",
        "        random.shuffle(idx)\n",
        "        s = idx[:K]\n",
        "        q = idx[K:K+max_query_per_class]\n",
        "        if len(s) > 0:\n",
        "            support_idx.extend(s)\n",
        "        if len(q) > 0:\n",
        "            query_idx.extend(q)\n",
        "    return {\"support_idx\": support_idx, \"query_idx\": query_idx}\n",
        "\n",
        "def compute_prototypes(emb, labels, idxs):\n",
        "    \"\"\"\n",
        "    emb: [N, d], labels: [N], idxs: list of indices for support\n",
        "    return: dict class_id -> prototype [d]\n",
        "    \"\"\"\n",
        "    proto = {}\n",
        "    if len(idxs) == 0:\n",
        "        return proto\n",
        "    sel_labels = labels[idxs]\n",
        "    classes = torch.unique(sel_labels).tolist()\n",
        "    for c in classes:\n",
        "        mask = (sel_labels == c).nonzero(as_tuple=True)[0]\n",
        "        cls_vecs = emb[idxs][mask]  # [Nc, d]\n",
        "        proto[c] = cls_vecs.mean(0)\n",
        "    return proto\n",
        "\n",
        "def classify_by_prototypes(emb, proto):\n",
        "    \"\"\"\n",
        "    emb: [Nq, d]\n",
        "    proto: dict class_id -> [d]\n",
        "    return: logits [Nq, C'] built from negative euclidean distance\n",
        "    \"\"\"\n",
        "    if len(proto) == 0:\n",
        "        raise ValueError(\"No prototypes found. Increase K or check labels.\")\n",
        "    classes = sorted(proto.keys())\n",
        "    P = torch.stack([proto[c] for c in classes], dim=0)  # [C', d]\n",
        "    # cosine or euclidean; we'll use cosine similarity as logits\n",
        "    emb_norm = F.normalize(emb, p=2, dim=-1)\n",
        "    P_norm = F.normalize(P, p=2, dim=-1)\n",
        "    logits = emb_norm @ P_norm.T  # [Nq, C']\n",
        "    return logits, classes\n",
        "\n",
        "def cross_graph_alignment_loss(disease_protos):\n",
        "    \"\"\"\n",
        "    Encourage disease prototypes across graphs to align.\n",
        "    disease_protos: list of dict {class_id -> prototype}\n",
        "    Loss = average pairwise distance between same class id across graphs\n",
        "    (class ids are local; for synthetic labels this is a soft heuristic).\n",
        "    \"\"\"\n",
        "    # In real data you'd align by shared diseases (semantic anchors).\n",
        "    # Here we softly align the mean prototype across graphs.\n",
        "    if len(disease_protos) < 2:\n",
        "        return torch.tensor(0., device=device)\n",
        "\n",
        "    # Compute global mean proto per graph (averaging all classes)\n",
        "    graph_means = []\n",
        "    for p in disease_protos:\n",
        "        if len(p) == 0:\n",
        "            continue\n",
        "        P = torch.stack(list(p.values()), dim=0)  # [C_g, d]\n",
        "        graph_means.append(P.mean(0))\n",
        "    if len(graph_means) < 2:\n",
        "        return torch.tensor(0., device=device)\n",
        "\n",
        "    loss = 0.\n",
        "    cnt = 0\n",
        "    for i in range(len(graph_means)):\n",
        "        for j in range(i+1, len(graph_means)):\n",
        "            loss = loss + F.mse_loss(graph_means[i], graph_means[j])\n",
        "            cnt += 1\n",
        "    return loss / max(cnt, 1)"
      ],
      "metadata": {
        "id": "hhrcRydJccMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) META-TRAIN / META-VAL / META-TEST SPLIT (by graphs)\n",
        "\n",
        "assert len(dgl_graphs) >= 5, \"Need 5 graphs for this split.\"\n",
        "train_graphs = dgl_graphs[:3]\n",
        "val_graphs   = [dgl_graphs[3]]\n",
        "test_graphs  = [dgl_graphs[4]]\n",
        "\n",
        "print(\"Meta-train:\", [g[\"graph_id\"] for g in train_graphs])\n",
        "print(\"Meta-val:  \", [g[\"graph_id\"] for g in val_graphs])\n",
        "print(\"Meta-test: \", [g[\"graph_id\"] for g in test_graphs])"
      ],
      "metadata": {
        "id": "D6Yp4jjVcss2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) TRAINING CONFIG\n",
        "\n",
        "K_SHOT = 3\n",
        "MAX_Q = 10\n",
        "EPISODES_PER_EPOCH = 15\n",
        "VAL_EPISODES = 5\n",
        "TEST_EPISODES = 15\n",
        "LR = 1e-3\n",
        "EPOCHS = 5\n",
        "ALIGN_COEF = 0.1  # weight for cross-graph alignment loss\n",
        "WD = 1e-5\n",
        "\n",
        "optimizer = torch.optim.AdamW(encoder.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "# Helper to run one episode on one graph\n",
        "def run_episode_on_graph(gpack, train_mode=True, k_shot=K_SHOT, max_q=MAX_Q):\n",
        "    g = gpack[\"g\"].to(device)\n",
        "    feats_dict = {nt: g.nodes[nt].data[\"h\"].to(device) for nt in g.ntypes}\n",
        "\n",
        "    # forward encoder\n",
        "    z_dict = encoder(g, feats_dict)  # {ntype: [N_t, d]}\n",
        "\n",
        "    # only classify disease nodes\n",
        "    d_labels = gpack[\"disease_labels\"][\"labels\"]\n",
        "    d_idxs   = gpack[\"disease_labels\"][\"idxs\"]\n",
        "\n",
        "    if d_labels is None or len(d_idxs) == 0:\n",
        "        return torch.tensor(0., device=device), 0., 0.  # no-op\n",
        "\n",
        "    d_emb = z_dict[\"disease\"]  # [Nd, d]\n",
        "    d_lab = d_labels.to(device)\n",
        "\n",
        "    # support/query\n",
        "    split = sample_support_query(d_lab, K=k_shot, max_query_per_class=max_q)\n",
        "    if len(split[\"support_idx\"]) == 0 or len(split[\"query_idx\"]) == 0:\n",
        "        return torch.tensor(0., device=device), 0., 0.\n",
        "\n",
        "    proto = compute_prototypes(d_emb, d_lab, split[\"support_idx\"])\n",
        "    q_emb = d_emb[split[\"query_idx\"]]\n",
        "    q_lab = d_lab[split[\"query_idx\"]]\n",
        "\n",
        "    logits, classes = classify_by_prototypes(q_emb, proto)\n",
        "    # Map q_lab to indices in 'classes'\n",
        "    class2pos = {c:i for i,c in enumerate(classes)}\n",
        "    y = torch.tensor([class2pos[int(c.cpu())] for c in q_lab], device=device)\n",
        "\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    # Accuracy\n",
        "    pred = logits.argmax(dim=1)\n",
        "    acc = (pred == y).float().mean().item()\n",
        "\n",
        "    return loss, acc, logits.size(0)\n",
        "\n",
        "def evaluate(graph_list, episodes=VAL_EPISODES, k_shot=K_SHOT, max_q=MAX_Q):\n",
        "    encoder.eval()\n",
        "    ep_losses, ep_accs, ep_counts = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(episodes):\n",
        "            # sample a random graph (task)\n",
        "            gp = random.choice(graph_list)\n",
        "            loss, acc, nq = run_episode_on_graph(gp, train_mode=False, k_shot=k_shot, max_q=max_q)\n",
        "            if nq > 0:\n",
        "                ep_losses.append(loss.item())\n",
        "                ep_accs.append(acc)\n",
        "                ep_counts.append(nq)\n",
        "    if len(ep_losses) == 0:\n",
        "        return 0., 0.\n",
        "    return float(np.mean(ep_losses)), float(np.mean(ep_accs))"
      ],
      "metadata": {
        "id": "Dj8Be8PbcySV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) TRAINING LOOP (meta-train on graphs; align across tasks)\n",
        "\n",
        "best_val = -1.0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    encoder.train()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for _ in tqdm(range(EPISODES_PER_EPOCH), desc=f\"Epoch {epoch}/{EPOCHS}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Optionally aggregate prototypes for alignment\n",
        "        disease_protos_all = []\n",
        "\n",
        "        # Sample a mini-batch of tasks (graphs)\n",
        "        tasks = [random.choice(train_graphs) for __ in range(3)]  # 3 tasks per episode\n",
        "        total_loss = torch.tensor(0., device=device)\n",
        "\n",
        "        for gp in tasks:\n",
        "            loss, acc, nq = run_episode_on_graph(gp, train_mode=True, k_shot=K_SHOT, max_q=MAX_Q)\n",
        "            if nq == 0:\n",
        "                continue\n",
        "            total_loss = total_loss + loss\n",
        "            running_loss += loss.item()\n",
        "            running_acc += acc\n",
        "            count += 1\n",
        "\n",
        "            # collect disease prototypes for alignment\n",
        "            # recompute embeddings for prototype extraction\n",
        "            g = gp[\"g\"].to(device)\n",
        "            feats_dict = {nt: g.nodes[nt].data[\"h\"].to(device) for nt in g.ntypes}\n",
        "            with torch.no_grad():\n",
        "                z = encoder(g, feats_dict)\n",
        "            d_labels = gp[\"disease_labels\"][\"labels\"]\n",
        "            d_idxs = gp[\"disease_labels\"][\"idxs\"]\n",
        "            if d_labels is not None and len(d_idxs) > 0:\n",
        "                split = sample_support_query(d_labels, K=K_SHOT, max_query_per_class=MAX_Q)\n",
        "                if len(split[\"support_idx\"]) > 0:\n",
        "                    p = compute_prototypes(z[\"disease\"], d_labels.to(device), split[\"support_idx\"])\n",
        "                    disease_protos_all.append(p)\n",
        "\n",
        "        # cross-graph alignment (encourages transferable disease structure)\n",
        "        if len(disease_protos_all) >= 2:\n",
        "            align_loss = cross_graph_alignment_loss(disease_protos_all)\n",
        "            total_loss = total_loss + ALIGN_COEF * align_loss\n",
        "\n",
        "        if total_loss.requires_grad:\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "    train_loss = running_loss / max(count, 1)\n",
        "    train_acc  = running_acc / max(count, 1)\n",
        "\n",
        "    val_loss, val_acc = evaluate(val_graphs, episodes=VAL_EPISODES, k_shot=K_SHOT, max_q=MAX_Q)\n",
        "\n",
        "    print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "    if val_acc > best_val:\n",
        "        best_val = val_acc\n",
        "        best_state = {k: v.cpu().clone() for k, v in encoder.state_dict().items()}\n",
        "\n",
        "# Load best model\n",
        "if best_state is not None:\n",
        "    encoder.load_state_dict(best_state)\n",
        "    print(f\"Loaded best validation model (Val Acc = {best_val:.3f}).\")"
      ],
      "metadata": {
        "id": "py9W1ssKdixQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) FINAL EVALUATION ON META-TEST GRAPH(S)\n",
        "\n",
        "test_loss, test_acc = evaluate(test_graphs, episodes=TEST_EPISODES, k_shot=K_SHOT, max_q=MAX_Q)\n",
        "print(f\"[META-TEST] Loss: {test_loss:.4f} | Acc: {test_acc:.3f}\""
      ],
      "metadata": {
        "id": "X-xB6Z8rdqav"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}